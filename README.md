# üè¶ Credit Risk Classification - Home Credit Default Risk

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![LightGBM](https://img.shields.io/badge/LightGBM-3.0+-green.svg)](https://lightgbm.readthedocs.io/)
[![Gradio](https://img.shields.io/badge/Gradio-4.0+-red.svg)](https://gradio.app/)

**Project**: Credit Risk Assessment & Scoring System  
**Author**: Rafsamjani Anugrah  
**Date**: November 2025  
**Dataset**: Home Credit Default Risk 

---

## üìã Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Technical Details](#technical-details)

---

## üéØ Overview

Sistem machine learning untuk **memprediksi risiko kredit** dan memberikan rekomendasi bisnis (Approve/Adjust/Reject) berdasarkan data aplikasi kredit dan histori pembayaran customer.

### Business Objectives
1. **Reduce default rate** dari ~8% menjadi <5%
2. **Increase approval rate** untuk customer berisiko rendah (segmen aman)
3. **Provide clear business rules** untuk keputusan kredit

### Key Metrics
- **ROC-AUC**: Kemampuan model memisahkan good/bad customer
- **KS-Statistic**: Standar industri credit scoring (target: >0.3)
- **Precision/Recall**: Balance antara risk dan opportunity
- **Approval Rate by Threshold**: Business impact measurement

---

## üìä Dataset

### Data Sources (8 Tables)
| Table | Records | Description |
|-------|---------|-------------|
| `application_train.csv` | 307,511 | Main training data dengan TARGET |
| `application_test.csv` | 48,744 | Test data untuk scoring |
| `bureau.csv` | 1.7M | Kredit eksternal dari credit bureau |
| `bureau_balance.csv` | 27M | Histori bulanan kredit eksternal |
| `previous_application.csv` | 1.6M | Aplikasi sebelumnya ke Home Credit |
| `installments_payments.csv` | 13M | Histori pembayaran cicilan |
| `POS_CASH_balance.csv` | 10M | Histori POS/Cash loans |
| `credit_card_balance.csv` | 3.8M | Histori kartu kredit |

### Target Distribution
- **TARGET = 0** (Good): 282,686 (91.9%)
- **TARGET = 1** (Bad): 24,825 (8.1%)
- **Imbalance Ratio**: 11.4:1

---

## üîß Methodology

### 1. **Exploratory Data Analysis (EDA)**
- ‚úÖ Missing value analysis (>80% missing ‚Üí dropped)
- ‚úÖ Outlier detection (DAYS_EMPLOYED: 365243 ‚Üí set NaN)
- ‚úÖ Correlation analysis
- ‚úÖ Distribution visualization

### 2. **Feature Engineering**
#### Application Features (Direct)
- `ANNUITY_INCOME_RATIO` = AMT_ANNUITY / AMT_INCOME_TOTAL
- `CREDIT_INCOME_RATIO` = AMT_CREDIT / AMT_INCOME_TOTAL
- `CREDIT_TERM` = AMT_CREDIT / AMT_ANNUITY
- `AGE` = -DAYS_BIRTH / 365
- `EMPLOYED_YEARS` = -DAYS_EMPLOYED / 365

#### Bureau Features (Aggregated)
- `BUREAU_NUM_ACTIVE`: Jumlah kredit eksternal aktif
- `BUREAU_SUM_DEBT`: Total utang di luar HC
- `BUREAU_MAX_DPD`: Keterlambatan terburuk
- `BUREAU_MONTHS_BAD_SHARE`: % bulan dengan status buruk

#### Previous Application Features
- `PREV_NUM_APPS`: Total aplikasi sebelumnya
- `PREV_APPROVAL_RATE`: Tingkat approval historis
- `PREV_LAST_DECISION_DAYS`: Jarak aplikasi terakhir

#### Payment Features
- `PAY_MAX_DPD`: Keterlambatan maksimal
- `PAY_PCT_LATE`: % cicilan yang telat
- `PAY_MEAN_DIFF`: Rata-rata over/underpayment

#### POS & Credit Card Features
- `CC_MEAN_UTILIZATION`: Rata-rata utilisasi CC
- `POS_MAX_DPD`: Keterlambatan POS loans

**Total Features**: ~150 features (after aggregation & selection)

### 3. **Data Preprocessing**

#### Handling Imbalance
```python
class_weight='balanced'  # For all models
# Weight = n_samples / (n_classes * n_class_samples)
# Class 0 weight: 0.545
# Class 1 weight: 6.199
```

#### Missing Value Imputation
- **Numeric**: Median imputation
- **Categorical**: Most frequent imputation
- **High missing (>80%)**: Dropped

#### Encoding
- **Categorical**: Label Encoding (for tree-based) + One-Hot (for linear)
- **Binary flags**: Already 0/1

#### Scaling
- **Numeric features**: StandardScaler (mean=0, std=1)
- **Applied to**: Logistic Regression, SVM, KNN only
- **Not applied to**: Tree-based models (DT, RF, LGBM)

#### Data Split
- **Train**: 70% (215,258 samples)
- **Validation**: 15% (46,127 samples)
- **Holdout Test**: 15% (46,126 samples)
- **Method**: Stratified split (maintain class ratio)

### 4. **Machine Learning Models**

#### 6 Algorithms Tested

| # | Model | Parameters | Why? |
|---|-------|------------|------|
| 1 | **Logistic Regression** | `max_iter=1000, class_weight='balanced'` | Baseline + explainable |
| 2 | **Decision Tree** | `max_depth=8, min_samples_leaf=50` | Interpretable rules |
| 3 | **Random Forest** | `n_estimators=100, max_depth=10` | Robust to noise |
| 4 | **LightGBM** | `n_estimators=100, learning_rate=0.05` | Best performance |
| 5 | **SVM** | `kernel='rbf', C=1.0` (20% sample) | Margin-based |
| 6 | **KNN** | `n_neighbors=15, weights='distance'` | Proximity-based |

#### Hyperparameter Rationale

**Logistic Regression**
```python
LogisticRegression(
    max_iter=1000,          # Convergence untuk large dataset
    class_weight='balanced', # Handle imbalance
    random_state=42
)
```

**Random Forest**
```python
RandomForestClassifier(
    n_estimators=100,       # Balance speed vs accuracy
    max_depth=10,           # Prevent overfitting
    min_samples_split=100,  # Min samples untuk split node
    min_samples_leaf=50,    # Min samples di leaf node
    class_weight='balanced',
    n_jobs=-1               # Parallel processing
)
```

**LightGBM** (Best Model)
```python
LGBMClassifier(
    n_estimators=100,       # Number of boosting rounds
    max_depth=8,            # Tree depth
    learning_rate=0.05,     # Slow learning = better generalization
    num_leaves=31,          # Max leaves per tree (2^depth - 1)
    class_weight='balanced',
    verbose=-1
)
```

### 5. **Model Evaluation**

#### Technical Metrics
- **ROC-AUC**: Area Under ROC Curve (0.5 = random, 1.0 = perfect)
- **KS-Statistic**: max(TPR - FPR) ‚Üí industry standard
- **Precision**: TP / (TP + FP) ‚Üí accuracy of positive predictions
- **Recall**: TP / (TP + FN) ‚Üí coverage of actual positives
- **F1-Score**: Harmonic mean of Precision & Recall

#### Threshold Analysis
| Threshold | Approval Rate | Precision (Bad) | Recall (Bad) | Business Meaning |
|-----------|---------------|-----------------|--------------|------------------|
| 0.3 | ~95% | Lower | Higher | Aggressive (risky) |
| 0.5 | ~88% | Medium | Medium | Balanced |
| 0.7 | ~75% | Higher | Lower | Conservative (safe) |

---

## üìà Results

### Model Performance Comparison

| Model | ROC-AUC | KS-Stat | Precision | Recall | F1-Score |
|-------|---------|---------|-----------|--------|----------|
| **LightGBM** | **0.76** | **0.42** | **0.45** | **0.68** | **0.54** |
| Random Forest | 0.74 | 0.39 | 0.43 | 0.65 | 0.52 |
| Logistic Regression | 0.72 | 0.35 | 0.38 | 0.62 | 0.47 |
| Decision Tree | 0.68 | 0.31 | 0.35 | 0.58 | 0.44 |
| KNN | 0.65 | 0.28 | 0.32 | 0.55 | 0.40 |
| SVM | 0.64 | 0.26 | 0.30 | 0.52 | 0.38 |

**Winner**: **LightGBM** dengan ROC-AUC 0.76 dan KS-Stat 0.42

### Feature Importance (Top 10)

1. **EXT_SOURCE_3** (0.156) ‚Üí External credit score 3
2. **EXT_SOURCE_2** (0.142) ‚Üí External credit score 2
3. **BUREAU_MAX_DPD** (0.089) ‚Üí Worst delay in external credit
4. **AGE** (0.078) ‚Üí Customer age
5. **ANNUITY_INCOME_RATIO** (0.071) ‚Üí DTI ratio
6. **CREDIT_INCOME_RATIO** (0.065) ‚Üí Credit/income ratio
7. **PAY_MAX_DPD** (0.058) ‚Üí Worst delay in HC payments
8. **PREV_APPROVAL_RATE** (0.052) ‚Üí Historical approval rate
9. **EMPLOYED_YEARS** (0.048) ‚Üí Employment stability
10. **BUREAU_SUM_DEBT** (0.045) ‚Üí Total external debt

### Business Rules Implementation

```python
if risk_score < 0.3:
    decision = "APPROVE"
    action = "Standard terms (100% credit, standard rate)"
    
elif risk_score < 0.6:
    decision = "APPROVE WITH ADJUSTMENT"
    action = """
    - Reduce credit by 20-30%, OR
    - Extend tenor (lower monthly payment), OR
    - Increase down payment by 10-20%
    """
    
else:
    decision = "REJECT"
    action = "Decline application / manual review for exceptions"
```

### Business Impact

#### Before Model (Manual Review)
- Approval Rate: 85%
- Default Rate: 8.1%
- Manual Review Time: 2-3 days

#### After Model (Threshold = 0.5)
- Approval Rate: 88% (+3%)
- **Predicted Default Rate: 4.2%** (-48% reduction)
- Processing Time: Real-time
- **False Positive Rate**: 12% (rejected good customers)
- **False Negative Rate**: 32% (approved bad customers)

#### Uplift Analysis
- **Additional Safe Approvals**: ~3,000 per month
- **Prevented Defaults**: ~1,200 per month
- **Estimated Cost Savings**: $2.5M per year (assuming $2,000 avg loss per default)

---

## üöÄ Installation

### Prerequisites
```bash
Python 3.8+
pip or conda
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Requirements.txt
```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
lightgbm>=3.3.0
imbalanced-learn>=0.9.0
matplotlib>=3.4.0
seaborn>=0.11.0
gradio>=4.0.0
joblib>=1.1.0
```

---

## üíª Usage

### 1. Run Full Pipeline (Notebook)
```bash
jupyter notebook test.ipynb
```
Run cells sequentially (1-24)

### 2. Launch Gradio Interface
After training models in notebook, run last cell:
```python
demo.launch(share=False, inbrowser=True)
```

Access at: `http://localhost:7860`

### 3. Predict New Application
```python
import joblib
import pandas as pd

# Load model & preprocessors
model = joblib.load('best_credit_model.pkl')
scaler = joblib.load('scaler.pkl')
feature_info = joblib.load('feature_info.pkl')

# Create input
input_data = pd.DataFrame({
    'AMT_INCOME_TOTAL': [150000],
    'AMT_CREDIT': [500000],
    'AMT_ANNUITY': [25000],
    'AGE': [35],
    # ... other features
})

# Predict
risk_score = model.predict_proba(input_data)[0, 1]
print(f"Risk Score: {risk_score:.2%}")
```

---

## üõ†Ô∏è Technical Details

### System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Data Sources (8 Tables)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Feature Engineering Pipeline            ‚îÇ
‚îÇ  - Aggregation (1-to-many ‚Üí 1-to-1)            ‚îÇ
‚îÇ  - Derived features (ratios, flags)             ‚îÇ
‚îÇ  - Missing value handling                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Preprocessing Pipeline               ‚îÇ
‚îÇ  - Encoding (Label/OneHot)                      ‚îÇ
‚îÇ  - Imputation (Median/Mode)                     ‚îÇ
‚îÇ  - Scaling (StandardScaler)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Model Training (6 Algorithms)          ‚îÇ
‚îÇ  - Stratified 70/15/15 split                    ‚îÇ
‚îÇ  - Class weight balancing                       ‚îÇ
‚îÇ  - Hyperparameter tuning                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Evaluation & Model Selection              ‚îÇ
‚îÇ  - ROC-AUC, KS-Stat, Precision/Recall          ‚îÇ
‚îÇ  - Threshold analysis                           ‚îÇ
‚îÇ  - Feature importance                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Deployment (Gradio Interface)           ‚îÇ
‚îÇ  - Real-time prediction                         ‚îÇ
‚îÇ  - Business rules engine                        ‚îÇ
‚îÇ  - Interactive web UI                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### File Structure
```
Rakamin_VIX_internship/
‚îú‚îÄ‚îÄ test.ipynb                    # Main notebook
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ PRESENTATION_GUIDE.md         # Presentation materials
‚îú‚îÄ‚îÄ requirements.txt              # Dependencies
‚îú‚îÄ‚îÄ application_train.csv         # Training data
‚îú‚îÄ‚îÄ application_test.csv          # Test data
‚îú‚îÄ‚îÄ bureau.csv                    # External credit data
‚îú‚îÄ‚îÄ bureau_balance.csv
‚îú‚îÄ‚îÄ previous_application.csv
‚îú‚îÄ‚îÄ installments_payments.csv
‚îú‚îÄ‚îÄ POS_CASH_balance.csv
‚îú‚îÄ‚îÄ credit_card_balance.csv
‚îú‚îÄ‚îÄ HomeCredit_columns_description.csv
‚îú‚îÄ‚îÄ best_credit_model.pkl         # Saved model
‚îú‚îÄ‚îÄ scaler.pkl                    # Saved scaler
‚îú‚îÄ‚îÄ num_imputer.pkl
‚îú‚îÄ‚îÄ cat_imputer.pkl
‚îú‚îÄ‚îÄ label_encoders.pkl
‚îî‚îÄ‚îÄ feature_info.pkl
```

---

## üìä Skills Demonstrated

### 1. **EDA (Exploratory Data Analysis)**
- ‚úÖ Missing value analysis
- ‚úÖ Outlier detection
- ‚úÖ Distribution analysis
- ‚úÖ Correlation heatmaps
- ‚úÖ Target imbalance check

### 2. **Data Visualization**
- ‚úÖ Distribution plots (histograms, KDE)
- ‚úÖ Boxplots for outliers
- ‚úÖ Correlation matrices
- ‚úÖ Feature importance charts
- ‚úÖ ROC curves

### 3. **Data Storytelling**
- ‚úÖ Business context explanation
- ‚úÖ Insight narration
- ‚úÖ Decision rationale
- ‚úÖ Impact quantification

### 4. **Feature Engineering**
- ‚úÖ Aggregation dari 1-to-many tables
- ‚úÖ Derived features (ratios, flags)
- ‚úÖ Domain knowledge application
- ‚úÖ Temporal features (days ‚Üí years)

### 5. **Feature Selection**
- ‚úÖ High missing value removal (>80%)
- ‚úÖ Low variance removal
- ‚úÖ Correlation-based filtering
- ‚úÖ Feature importance ranking

### 6. **Machine Learning Modelling**
- ‚úÖ 6 algorithms comparison
- ‚úÖ Hyperparameter tuning
- ‚úÖ Cross-validation
- ‚úÖ Ensemble methods
- ‚úÖ Imbalance handling

### 7. **Business Acumen**
- ‚úÖ Threshold analysis untuk business rules
- ‚úÖ Cost-benefit analysis
- ‚úÖ Approval rate optimization
- ‚úÖ Risk-opportunity balance
- ‚úÖ Actionable recommendations

---

## üìÑ License

This project is licensed under the MIT License.

---

## üôè Acknowledgments

- **Dataset**: Home Credit Default Risk 
- **Framework**: scikit-learn, LightGBM, Gradio
- **Inspiration**: Real-world credit scoring systems

---

**‚≠ê Star this repo if you find it helpful!**
